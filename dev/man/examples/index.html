<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Examples · GEEBRA</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">GEEBRA</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li class="is-active"><a class="tocitem" href>Examples</a><ul class="internal"><li><a class="tocitem" href="#Contents-1"><span>Contents</span></a></li><li><a class="tocitem" href="#Ratio-of-two-means-1"><span>Ratio of two means</span></a></li><li><a class="tocitem" href="#Logistic-regression-1"><span>Logistic regression</span></a></li></ul></li><li><span class="tocitem">Documentation</span><ul><li><a class="tocitem" href="../../lib/public/">Public</a></li><li><a class="tocitem" href="../../lib/internal/">Internal</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Examples</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/ikosmidis/GEEBRA.jl/blob/master/docs/src/man/examples.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Examples-1"><a class="docs-heading-anchor" href="#Examples-1">Examples</a><a class="docs-heading-anchor-permalink" href="#Examples-1" title="Permalink"></a></h1><h2 id="Contents-1"><a class="docs-heading-anchor" href="#Contents-1">Contents</a><a class="docs-heading-anchor-permalink" href="#Contents-1" title="Permalink"></a></h2><ul><li><a href="#Examples-1">Examples</a></li><ul><li><a href="#Contents-1">Contents</a></li><li><a href="#Ratio-of-two-means-1">Ratio of two means</a></li><li><a href="#Logistic-regression-1">Logistic regression</a></li><ul><li><a href="#Using-[objective_function_template](@ref)-1">Using <code>objective_function_template</code></a></li><li><a href="#Using-[estimating_function_template](@ref)-1">Using <code>estimating_function_template</code></a></li><li><a href="#Bias-reduction-methods-1">Bias-reduction methods</a></li></ul></ul></ul><h2 id="Ratio-of-two-means-1"><a class="docs-heading-anchor" href="#Ratio-of-two-means-1">Ratio of two means</a><a class="docs-heading-anchor-permalink" href="#Ratio-of-two-means-1" title="Permalink"></a></h2><p>Consider a setting where independent pairs of random variables <span>$(X_1, Y_1), \ldots, (X_n, Y_n)$</span> are observed, and suppose that interest is in the ratio of the mean of <span>$Y_i$</span> to  the mean of <span>$X_i$</span>, that is <span>$\theta = \mu_Y / \mu_X$</span>, with   <span>$\mu_X = E(X_i)$</span> and <span>$\mu_Y = E(Y_i) \ne 0$</span> <span>$(i = 1, \ldots, n)$</span>.</p><p>Assuming that sampling is from an infinite population, one way of estimating <span>$\theta$</span> without any further assumptions about the joint distribution of <span>$(X_i, Y_i)$</span> is to set the unbiased estimating equation <span>$\sum_{i = 1}^n (Y_i - \theta X_i) = 0$</span>. The resulting <span>$M$</span>-estimator is then  <span>$\hat\theta = s_Y/s_X$</span> where <span>$s_X = \sum_{i = 1}^n X_i$</span> and <span>$s_Y = \sum_{i = 1}^n Y_i$</span>. </p><p>The estimator <span>$\hat\theta$</span> is generally biased, as can be shown, for example, by an application of the Jensen inequality assuming that <span>$X_i$</span>is independent of <span>$Y_i$</span>, and its bias can be reduced using the empirically adjusted estimating functions approach in Kosmidis &amp; Lunardon (2020). </p><p>This example illustrates how GEEBRA can be used to calculate the <span>$M$</span>-estimator and its reduced-bias version.</p><pre><code class="language-julia-repl">julia&gt; using GEEBRA, Random</code></pre><p>Define a data type for ratio estimation problems</p><pre><code class="language-julia-repl">julia&gt; struct ratio_data
           y::Vector
           x::Vector
       end;</code></pre><p>Write a function to compute the number of observations for objects of type <code>ratio_data</code>.</p><pre><code class="language-julia-repl">julia&gt; function ratio_nobs(data::ratio_data)
           nx = length(data.x)
           ny = length(data.y)
           if (nx != ny)
               error(&quot;length of x is not equal to the length of y&quot;)
           end
           nx
       end;</code></pre><p>Generate some data to test things out</p><pre><code class="language-julia-repl">julia&gt; Random.seed!(123);

julia&gt; my_data = ratio_data(randn(10), rand(10));

julia&gt; ratio_nobs(my_data)
10</code></pre><p>The estimating function for the ratio <span>$\theta$</span> is </p><p><span>$\sum_{i = 1}^n (Y_i - \theta X_i)$</span></p><p>So, the contribution to the estimating function can be implemented as</p><pre><code class="language-julia-repl">julia&gt; function ratio_ef(theta::Vector,
                         data::ratio_data,
                         i::Int64)
           data.y[i] .- theta * data.x[i]
       end;</code></pre><p>The <code>estimating_function_template</code> for the ratio estimation problem can now be set up using <code>ratio_nobs</code> and <code>ratio_ef</code>.</p><pre><code class="language-julia-repl">julia&gt;     ratio_template = estimating_function_template(ratio_nobs, ratio_ef);</code></pre><p>We are now ready use <code>ratio_template</code> and <code>my_data</code> to compute the <span>$M$</span>-estimator of <span>$\theta$</span> by solving the esitmating equation <span>$\sum_{i = 1}^n (Y_i - \theta X_i) = 0$</span>. The starting value for the nonlinear solver is set to <code>0.1</code>.</p><pre><code class="language-julia-repl">julia&gt; result_m = fit(ratio_template, my_data, [0.1])
M-estimation with estimating function contributions ratio_ef

────────────────────────────────────────────────────────────────────────
          Estimate  Std. Error  z value  Pr(&gt;|z|)   Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────
theta[1]   1.07548    0.573615  1.87492    0.0608  -0.0487819    2.19975
────────────────────────────────────────────────────────────────────────
Estimating functions:	[-2.051803171809752e-12]</code></pre><p><code>fit</code> uses methods from the <a href="https://github.com/JuliaNLSolvers/NLsolve.jl"><strong>NLsolve</strong></a> package for solving the estimating equations. Arguments can be passed directly to <code>NLsolve.nlsolve</code> through <a href="https://docs.julialang.org/en/v1/manual/functions/#Keyword-Arguments-1">keyword arguments</a> to the <code>fit</code> method. For example,</p><pre><code class="language-julia-repl">julia&gt; result_m = fit(ratio_template, my_data, [0.1], show_trace = true)
Iter     f(x) inf-norm    Step 2-norm 
------   --------------   --------------
     0     4.321212e+00              NaN
     1     3.878230e+00     1.000000e-01
     2     2.992266e+00     2.000000e-01
     3     1.220339e+00     4.000000e-01
     4     2.051803e-12     2.754829e-01
M-estimation with estimating function contributions ratio_ef

────────────────────────────────────────────────────────────────────────
          Estimate  Std. Error  z value  Pr(&gt;|z|)   Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────
theta[1]   1.07548    0.573615  1.87492    0.0608  -0.0487819    2.19975
────────────────────────────────────────────────────────────────────────
Estimating functions:	[-2.051803171809752e-12]</code></pre><p>Bias reduction in general <span>$M$</span>-estimation can be achieved by solving the adjusted estimating equation <span>$\sum_{i = 1}^n (Y_i - \theta X_i) + A(\theta, Y, X) = 0$</span>, where <span>$A(\theta)$</span> are empirical bias-reducing adjustments depending on the first and second derivatives of the estimating function contributions. <strong>GEEBRA</strong> can use <code>ratio_template</code> and automatic differentiation (see, <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff</a>) to construct <span>$A(\theta, Y, X)$</span> and, then, solve the bias-reducing adjusted estimating equations. All this is simply done by</p><pre><code class="language-julia-repl">julia&gt; result_br = fit(ratio_template, my_data, [0.1], estimation_method = &quot;RBM&quot;)
RBM-estimation with estimating function contributions ratio_ef
Bias reduction method: implicit_trace

────────────────────────────────────────────────────────────────────────
          Estimate  Std. Error  z value  Pr(&gt;|z|)   Lower 95%  Upper 95%
────────────────────────────────────────────────────────────────────────
theta[1]   1.06754    0.573499  1.86146    0.0627  -0.0564928    2.19158
────────────────────────────────────────────────────────────────────────
Adjusted estimating functions:	[-1.3371248552829229e-14]</code></pre><p>where <code>RBM</code> stands for reduced-bias <code>M</code>-estimation.</p><p>Kosmidis &amp; Lunardon (2020) show that the reduced-bias estimator of <span>$\theta$</span> is <span>$\tilde\theta = (s_Y + s_{XY}/s_{X})/(s_X + s_{XX}/s_{X})$</span>. The code chunks below tests that this is indeed the result <strong>GEEBRA</strong> returns.</p><pre><code class="language-julia-repl">julia&gt; sx = sum(my_data.x);

julia&gt; sxx = sum(my_data.x .* my_data.x);

julia&gt; sy = sum(my_data.y);

julia&gt; sxy = sum(my_data.x .* my_data.y);

julia&gt; isapprox(sy/sx, result_m.theta[1])
true

julia&gt; isapprox((sy + sxy/sx)/(sx + sxx/sx), result_br.theta[1])
true</code></pre><h2 id="Logistic-regression-1"><a class="docs-heading-anchor" href="#Logistic-regression-1">Logistic regression</a><a class="docs-heading-anchor-permalink" href="#Logistic-regression-1" title="Permalink"></a></h2><h3 id="Using-[objective_function_template](@ref)-1"><a class="docs-heading-anchor" href="#Using-[objective_function_template](@ref)-1">Using <a href="../../lib/public/#GEEBRA.objective_function_template"><code>objective_function_template</code></a></a><a class="docs-heading-anchor-permalink" href="#Using-[objective_function_template](@ref)-1" title="Permalink"></a></h3><p>Here, we use <strong>GEEBRA</strong>&#39;s <a href="../../lib/public/#GEEBRA.objective_function_template"><code>objective_function_template</code></a> to estimate a logistic regression model using maximum likelihood and maximum penalized likelihood, with the empirical bias-reducing penalty in Kosmidis &amp; Lunardon (2020).</p><pre><code class="language-julia-repl">julia&gt; using GEEBRA

julia&gt; using Random

julia&gt; using Distributions

julia&gt; using Optim</code></pre><p>A data type for logistic regression models (consisting of a response vector <code>y</code>, a model matrix <code>x</code>, and a vector of weights <code>m</code>) is</p><pre><code class="language-julia-repl">julia&gt; struct logistic_data
           y::Vector
           x::Array{Float64}
           m::Vector
       end</code></pre><p>A function to compute the number of observations from <code>logistic_data</code> objects is</p><pre><code class="language-julia-repl">julia&gt; function logistic_nobs(data::logistic_data)
           nx = size(data.x)[1]
           ny = length(data.y)
           nm = length(data.m)
           if (nx != ny)
               error(&quot;number of rows in of x is not equal to the length of y&quot;)
           elseif (nx != nm)
               error(&quot;number of rows in of x is not equal to the length of m&quot;)
           elseif (ny != nm)
               error(&quot;length of y is not equal to the length of m&quot;)
           end
           nx
       end
logistic_nobs (generic function with 1 method)</code></pre><p>The logistic regression log-likelihood contribution at a parameter <code>theta</code> for the <span>$i$</span>th observations of data <code>data</code> is</p><pre><code class="language-julia-repl">julia&gt; function logistic_loglik(theta::Vector,
                                data::logistic_data,
                                i::Int64)
           eta = sum(data.x[i, :] .* theta)
           mu = exp.(eta)./(1 .+ exp.(eta))
           data.y[i] .* log.(mu) + (data.m[i] - data.y[i]) .* log.(1 .- mu)
       end
logistic_loglik (generic function with 1 method)</code></pre><p>Let&#39;s simulate some logistic regression data with <span>$10$</span> covariates</p><pre><code class="language-julia-repl">julia&gt; Random.seed!(123);

julia&gt; n = 100;

julia&gt; m = 1;

julia&gt; p = 10
10

julia&gt; x = Array{Float64}(undef, n, p);

julia&gt; x[:, 1] .= 1.0;

julia&gt; for j in 2:p
               x[:, j] .= rand(n);
       end

julia&gt; true_betas = randn(p) * sqrt(p);

julia&gt; y = rand.(Binomial.(m, cdf.(Logistic(), x * true_betas)));

julia&gt; my_data = logistic_data(y, x, fill(m, n));</code></pre><p>and set up an <code>objective_function_template</code> for logistic regression</p><pre><code class="language-julia-repl">julia&gt; logistic_template = objective_function_template(logistic_nobs, logistic_loglik)
objective_function_template(Main.ex-2.logistic_nobs, Main.ex-2.logistic_loglik)</code></pre><p>The maximum likelihood estimates starting at <code>true_betas</code> are</p><pre><code class="language-julia-repl">julia&gt; o1_ml = fit(logistic_template, my_data, true_betas, optim_method = NelderMead())
M-estimation with objective contributions logistic_loglik

──────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error   z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
──────────────────────────────────────────────────────────────────────────
theta[1]    1.03786      2.46744   0.42062    0.6740   -3.79824   5.87395
theta[2]   -6.33539      1.53213  -4.13502    &lt;1e-4    -9.33832  -3.33247
theta[3]   -6.76459      1.63753  -4.13096    &lt;1e-4    -9.9741   -3.55508
theta[4]    0.994292     1.1077    0.89762    0.3694   -1.17676   3.16534
theta[5]    5.4302       1.6185    3.35508    0.0008    2.25799   8.6024
theta[6]    6.58354      1.92567   3.41884    0.0006    2.8093   10.3578
theta[7]    4.92474      1.39467   3.53111    0.0004    2.19123   7.65824
theta[8]    1.29824      1.25277   1.0363     0.3001   -1.15714   3.75362
theta[9]   -3.00981      1.43218  -2.10156    0.0356   -5.81682  -0.202796
theta[10]  -4.96494      1.85256  -2.68005    0.0074   -8.59589  -1.334
──────────────────────────────────────────────────────────────────────────
Maximum objetive:		-31.9044
Takeuchi information criterion:	83.6381
Akaike information criterion:	83.8088</code></pre><p><code>fit</code> uses methods from the <a href="https://github.com/JuliaNLSolvers/Optim.jl"><strong>Optim</strong></a> package internally. Here, we used the <code>Optim.NelderMead</code> method. Alternative optimization methods and options can be supplied directly through the <a href="https://docs.julialang.org/en/v1/manual/functions/#Keyword-Arguments-1">keyword arguments</a> <code>method</code> and <code>optim.Options</code>, respectively. For example,</p><pre><code class="language-julia-repl">julia&gt; o2_ml = fit(logistic_template, my_data, true_betas, optim_method = LBFGS(), optim_options = Optim.Options(g_abstol = 1e-05))
M-estimation with objective contributions logistic_loglik

───────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error    z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────
theta[1]    1.03787      2.46744   0.420628    0.6740   -3.79822   5.87397
theta[2]   -6.33546      1.53214  -4.13505     &lt;1e-4    -9.3384   -3.33253
theta[3]   -6.76458      1.63752  -4.13098     &lt;1e-4    -9.97407  -3.55509
theta[4]    0.994237     1.1077    0.897571    0.3694   -1.17681   3.16528
theta[5]    5.43023      1.61849   3.35512     0.0008    2.25805   8.60241
theta[6]    6.58354      1.92566   3.41885     0.0006    2.80932  10.3578
theta[7]    4.9247       1.39466   3.53111     0.0004    2.19122   7.65818
theta[8]    1.29827      1.25278   1.03632     0.3001   -1.15712   3.75367
theta[9]   -3.00978      1.43216  -2.10156     0.0356   -5.81676  -0.202795
theta[10]  -4.96494      1.85254  -2.68006     0.0074   -8.59585  -1.33402
───────────────────────────────────────────────────────────────────────────
Maximum objetive:		-31.9044
Takeuchi information criterion:	83.6381
Akaike information criterion:	83.8088</code></pre><p>The reduced-bias estimates starting at the maximum likelihood ones are</p><pre><code class="language-julia-repl">julia&gt; o1_br = fit(logistic_template, my_data, coef(o1_ml), estimation_method = &quot;RBM&quot;)
RBM-estimation with objective contributions logistic_loglik
Bias reduction method: implicit_trace

───────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error    z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────
theta[1]    0.928169    2.20201    0.42151     0.6734   -3.38769   5.24402
theta[2]   -5.4578      1.20924   -4.51341     &lt;1e-5    -7.82787  -3.08773
theta[3]   -5.81835     1.26797   -4.58872     &lt;1e-5    -8.30352  -3.33318
theta[4]    0.81906     0.980035   0.835746    0.4033   -1.10177   2.73989
theta[5]    4.64568     1.30015    3.5732      0.0004    2.09744   7.19393
theta[6]    5.66444     1.54657    3.66258     0.0002    2.63321   8.69566
theta[7]    4.16222     1.11835    3.72174     0.0002    1.97028   6.35415
theta[8]    1.14271     1.13618    1.00575     0.3145   -1.08416   3.36957
theta[9]   -2.58574     1.2083    -2.13999     0.0324   -4.95396  -0.217525
theta[10]  -4.22949     1.49518   -2.82876     0.0047   -7.15999  -1.299
───────────────────────────────────────────────────────────────────────────
Maximum penalized objetive:	-36.5538
Takeuchi information criterion:	81.9311
Akaike information criterion:	84.284</code></pre><h3 id="Using-[estimating_function_template](@ref)-1"><a class="docs-heading-anchor" href="#Using-[estimating_function_template](@ref)-1">Using <a href="../../lib/public/#GEEBRA.estimating_function_template"><code>estimating_function_template</code></a></a><a class="docs-heading-anchor-permalink" href="#Using-[estimating_function_template](@ref)-1" title="Permalink"></a></h3><p>The same results as above can be returned using an <a href="../../lib/public/#GEEBRA.estimating_function_template"><code>estimating_function_template</code></a> for logistic regression. </p><p>The contribution to the derivatives of the log-likelihood for logistic regression is</p><pre><code class="language-julia-repl">julia&gt; function logistic_ef(theta::Vector,
                            data::logistic_data,
                            i::Int64)
           eta = sum(data.x[i, :] .* theta)
           mu = exp.(eta)./(1 .+ exp.(eta))
           data.x[i, :] * (data.y[i] - data.m[i] * mu)
       end
logistic_ef (generic function with 1 method)</code></pre><p>Then, solving the bias-reducing adjusted estimating equations</p><pre><code class="language-julia-repl">julia&gt; logistic_template_ef = estimating_function_template(logistic_nobs, logistic_ef);

julia&gt; e1_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = &quot;RBM&quot;)
RBM-estimation with estimating function contributions logistic_ef
Bias reduction method: implicit_trace

───────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error    z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────
theta[1]    0.928169    2.20201    0.42151     0.6734   -3.38769   5.24402
theta[2]   -5.4578      1.20924   -4.51341     &lt;1e-5    -7.82787  -3.08773
theta[3]   -5.81835     1.26797   -4.58872     &lt;1e-5    -8.30352  -3.33318
theta[4]    0.81906     0.980035   0.835746    0.4033   -1.10177   2.73989
theta[5]    4.64568     1.30015    3.5732      0.0004    2.09744   7.19393
theta[6]    5.66444     1.54657    3.66258     0.0002    2.63321   8.69566
theta[7]    4.16222     1.11835    3.72174     0.0002    1.97028   6.35415
theta[8]    1.14271     1.13618    1.00575     0.3145   -1.08416   3.36957
theta[9]   -2.58574     1.2083    -2.13999     0.0324   -4.95396  -0.217525
theta[10]  -4.22949     1.49518   -2.82876     0.0047   -7.15999  -1.299
───────────────────────────────────────────────────────────────────────────
Adjusted estimating functions:	[-5.189099150371135e-12; -7.72212849220466e-12; … ; -5.199951580436846e-12; -4.3689080131414926e-12]</code></pre><p>returns the reduced-bias estimates from maximum penalized likelihood:</p><pre><code class="language-julia-repl">julia&gt; isapprox(coef(o1_br), coef(e1_br))
true</code></pre><h3 id="Bias-reduction-methods-1"><a class="docs-heading-anchor" href="#Bias-reduction-methods-1">Bias-reduction methods</a><a class="docs-heading-anchor-permalink" href="#Bias-reduction-methods-1" title="Permalink"></a></h3><p><strong>GEEBRA</strong> currently implements 2 alternative bias reduction methods, called <code>implicit_trace</code> and <code>explicit_trace</code>. <code>implicit_trace</code> will adjust the estimating functions or penalize the objectives, as we have seen earlier. <code>explicit_trace</code>, on the other hand, will form an estimate of the bias of the <span>$M$</span>-estimator and subtract that from the <span>$M$</span>-estimates. The default method is <code>implicit_trace</code>.</p><p>For example, for logistic regression via estimating functions </p><pre><code class="language-julia-repl">julia&gt; e2_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = &quot;RBM&quot;, br_method = &quot;explicit_trace&quot;)
RBM-estimation with estimating function contributions logistic_ef
Bias reduction method: explicit_trace

───────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error    z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────
theta[1]    0.895301    2.10014    0.426305    0.6699   -3.2209    5.0115
theta[2]   -5.11645     1.11115   -4.60463     &lt;1e-5    -7.29427  -2.93863
theta[3]   -5.46494     1.15431   -4.73437     &lt;1e-5    -7.72736  -3.20253
theta[4]    0.74801     0.936884   0.798402    0.4246   -1.08825   2.58427
theta[5]    4.34256     1.20242    3.6115      0.0003    1.98585   6.69926
theta[6]    5.29117     1.42302    3.71828     0.0002    2.50211   8.08023
theta[7]    3.87517     1.0339     3.74812     0.0002    1.84877   5.90158
theta[8]    1.07407     1.09374    0.98202     0.3261   -1.06962   3.21776
theta[9]   -2.42976     1.13518   -2.14042     0.0323   -4.65466  -0.204855
theta[10]  -3.93176     1.37586   -2.85768     0.0043   -6.62839  -1.23512
───────────────────────────────────────────────────────────────────────────
Adjusted estimating functions:	[0.04705609374706787; -0.006424930231943682; … ; -0.023832956574366418; -0.03494242367189526]</code></pre><p>which gives slightly different estimates that what are in the <code>implict_trace</code> fit in <code>e1_br</code>. </p><p>The same can be done using objective functions, but numerical differentiation (using the <a href="https://github.com/JuliaDiff/FiniteDiff.jl">FiniteDiff</a> package) is used to approximate the gradient of the bias-reducing penalty (i.e. <span>$A(\theta)$</span>).</p><pre><code class="language-julia-repl">julia&gt; o2_br = fit(logistic_template, my_data, true_betas, estimation_method = &quot;RBM&quot;, br_method = &quot;explicit_trace&quot;)
RBM-estimation with objective contributions logistic_loglik
Bias reduction method: explicit_trace

───────────────────────────────────────────────────────────────────────────
            Estimate  Std. Error    z value  Pr(&gt;|z|)  Lower 95%  Upper 95%
───────────────────────────────────────────────────────────────────────────
theta[1]    0.895301    2.10014    0.426305    0.6699   -3.2209    5.0115
theta[2]   -5.11645     1.11115   -4.60463     &lt;1e-5    -7.29427  -2.93863
theta[3]   -5.46494     1.15431   -4.73437     &lt;1e-5    -7.72736  -3.20253
theta[4]    0.74801     0.936884   0.798402    0.4246   -1.08825   2.58427
theta[5]    4.34256     1.20242    3.6115      0.0003    1.98585   6.69926
theta[6]    5.29117     1.42302    3.71828     0.0002    2.50211   8.08023
theta[7]    3.87517     1.0339     3.74812     0.0002    1.84877   5.90158
theta[8]    1.07407     1.09374    0.98202     0.3261   -1.06962   3.21776
theta[9]   -2.42976     1.13518   -2.14042     0.0323   -4.65466  -0.204855
theta[10]  -3.93176     1.37586   -2.85768     0.0043   -6.62839  -1.23512
───────────────────────────────────────────────────────────────────────────
Maximum penalized objetive:	-36.607
Takeuchi information criterion:	81.6646
Akaike information criterion:	84.7635

julia&gt; isapprox(coef(e2_br), coef(o2_br))
true</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../../lib/public/">Public »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 3 April 2020 13:54">Friday 3 April 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
