var documenterSearchIndex = {"docs":
[{"location":"lib/public/#Public-documentation-1","page":"Public","title":"Public documentation","text":"","category":"section"},{"location":"lib/public/#Contents-1","page":"Public","title":"Contents","text":"","category":"section"},{"location":"lib/public/#","page":"Public","title":"Public","text":"Pages = [\"public.md\"]","category":"page"},{"location":"lib/public/#Index-1","page":"Public","title":"Index","text":"","category":"section"},{"location":"lib/public/#","page":"Public","title":"Public","text":"Pages = [\"public.md\"]","category":"page"},{"location":"lib/public/#Public-interface-1","page":"Public","title":"Public interface","text":"","category":"section"},{"location":"lib/public/#","page":"Public","title":"Public","text":"estimating_function_template\nget_estimating_function\nestimating_function\nobjective_function_template\nobjective_function\nfit(template::objective_function_template, data::Any, theta::Vector; estimation_method::String = \"M\", br_method::String = \"implicit_trace\", optim_method = LBFGS(), optim_options = Optim.Options())\nfit(template::estimating_function_template, data::Any, theta::Vector; estimation_method::String = \"M\", br_method::String = \"implicit_trace\", nlsolve_arguments...)\ncoef\nvcov\nstderror\ncoeftable\ntic\naic","category":"page"},{"location":"lib/public/#GEEBRA.estimating_function_template","page":"Public","title":"GEEBRA.estimating_function_template","text":"estimating_function_template(nobs::Function, ef_contribution::Function)\n\nDefine an estimating_function_template by supplying:\n\nnobs: a function of data that computes the number of observations of the particular data type,\nef_contribution: a function of the parameters theta, the data and the observation index i that returns a vector of length length(theta).\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GEEBRA.get_estimating_function","page":"Public","title":"GEEBRA.get_estimating_function","text":"get_estimating_function(data::Any, template::estimating_function_template, br::Bool = false)\n\nConstruct the estimating function by adding up all contributions in the data according to estimating_function_template. If br = true then automatic differentiation is used to compute the empirical bias-reducing adjustments and add them to the estimating function. The result is a function that stores the estimating functions values at its second argument, in a preallocated vector passed as its first argument, ready to be used withing NLsolve.nlsolve.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#GEEBRA.estimating_function","page":"Public","title":"GEEBRA.estimating_function","text":"estimating_function(theta::Vector, data::Any, template::estimating_function_template, br::Bool = false)\n\nConstruct the estimating function by adding up all contributions in the data according to estimating_function_template, and evaluate it at theta. If br = true then automatic differentiation is used to compute the empirical bias-reducing adjustments and add them to the estimating function.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#GEEBRA.objective_function_template","page":"Public","title":"GEEBRA.objective_function_template","text":"objective_function_template(nobs::Function, obj_contribution::Function)\n\nDefine an objective_function_template by supplying:\n\nnobs: a function of data that computes the number of observations of the particular data type,\nobj_contribution: a function of the parameters theta, the data and the observation index i that returns a real.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GEEBRA.objective_function","page":"Public","title":"GEEBRA.objective_function","text":"objective_function(theta::Vector, data::Any, template::objective_function_template, br::Bool = false)\n\nConstruct the objective function by adding up all contributions in the data according to objective_function_template, and evaluate it at theta. If br = true then automatic differentiation is used to compute the empirical bias-reducing penalty and add it to the objective function.  \n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsBase.fit-Tuple{objective_function_template,Any,Array{T,1} where T}","page":"Public","title":"StatsBase.fit","text":"fit(template::objective_function_template, data::Any, theta::Vector; estimation_method::String = \"M\", br_method::String = \"implicit_trace\", optim_method = LBFGS(), optim_options = Optim.Options())\n\nFit an objective_function_template on data using M-estimation (keyword argument estimation_method = \"M\"; default) or RBM-estimation (reduced-bias M estimation; Kosmidis & Lunardon, 2020; keyword argument estimation_method = \"RBM\"). Bias reduction is either through the maximization of the bias-reducing penalized objective in Kosmidis & Lunardon (2020) (keyword argument br_method = \"implicit_trace\"; default) or by subtracting an estimate of the bias from the M-estimates (keyword argument br_method = \"explicit_trace\"). The bias-reducing penalty is constructed internally using automatic differentiation (using the ForwardDiff package), and the bias estimate using a combination of automatic differentiation (using the ForwardDiff package) and numerical differentiation (using the FiniteDiff package).\n\nThe maximization of the objective or the penalized objective is done using the Optim package. Optimization methods and options can be supplied directly through the keyword arguments optim_method and optim_options, respectively. optim_options expects an object of class Optim.Options. See the Optim documentation for more details on the available options.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#StatsBase.fit-Tuple{estimating_function_template,Any,Array{T,1} where T}","page":"Public","title":"StatsBase.fit","text":"fit(template::estimating_function_template, data::Any, theta::Vector; estimation_method::String = \"M\", br_method::String = \"implicit_trace\", nlsolve_arguments...)\n\nFit an estimating_function_template on data using M-estimation (keyword argument estimation_method = \"M\"; default) or RBM-estimation (reduced-bias M estimation; Kosmidis & Lunardon, 2020; keyword argument estimation_method = \"RBM\"). Bias reduction is either through the solution of the empirically adjusted estimating functions in Kosmidis & Lunardon (2020) (keyword argument br_method = \"implicit_trace\"; default) or by subtracting an estimate of the bias from the M-estimates (keyword argument br_method = \"explicit_trace\"). The bias-reducing adjustments and the bias estimate are constructed internally using automatic differentiation (using the ForwardDiff package).\n\nThe solution of the estimating equations or the adjusted estimating equations is done using the NLsolve package. Arguments can be passed directly to NLsolve.nlsolve through keyword arguments. See the NLsolve README for more information on available options.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#StatsBase.coef","page":"Public","title":"StatsBase.coef","text":"coef(results::GEEBRA_results)\n\nExtract the M-estimates or their reduced-bias versions from the output of fit for an objective_function_template or an estimating_function_template.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsBase.vcov","page":"Public","title":"StatsBase.vcov","text":"vcov(results::GEEBRA_results)\n\nCompute an esitmate of the variance-covariance matrix of the M-estimator or its reduced-bias version from the output of fit for an objective_function_template or an estimating_function_template.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsBase.stderror","page":"Public","title":"StatsBase.stderror","text":"stderror(results::GEEBRA_results)\n\nCompute esitmated standard errors for the M-estimator or its reduced-bias version from the output of fit for an objective_function_template or an estimating_function_template.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsBase.coeftable","page":"Public","title":"StatsBase.coeftable","text":"coeftable(results::GEEBRA_results; level::Real=0.95)\n\nReturn a StatsBase.CoefTable for the M-estimator or its reduced-bias version from the output of fit for an objective_function_template or an estimating_function_template. level can be used to set the level of the reported Wald-type confidence intervals (using quantiles of the standard normal distribution). \n\n\n\n\n\n","category":"function"},{"location":"lib/public/#GEEBRA.tic","page":"Public","title":"GEEBRA.tic","text":"tic(results::GEEBRA_results)\n\nCompute the Takeuchi Information Criterion at the M-estimator or its reduced-bias version from the output of fit for an objective_function_template. nothing is returned if results is the output of fit for an estimating_function_template.\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#StatsBase.aic","page":"Public","title":"StatsBase.aic","text":"aic(results::GEEBRA_results)\n\nCompute the Akaike Information Criterion at the M-estimator or its reduced-bias version from the output of fit for an objective_function_template. nothing is returned if results is the output of fit for an estimating_function_template.\n\n\n\n\n\n","category":"function"},{"location":"#[GEEBRA.jl](https://github.com/ikosmidis/GEEBRA.jl)-1","page":"Home","title":"GEEBRA.jl","text":"","category":"section"},{"location":"#Authors-1","page":"Home","title":"Authors","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Ioannis Kosmidis (author, maintainer)\nNicola Lunardon (author)","category":"page"},{"location":"#Licence-1","page":"Home","title":"Licence","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"MIT License","category":"page"},{"location":"#Package-description-1","page":"Home","title":"Package description","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"GEEBRA is a Julia package that implements M-estimation for statistical models, either by solving estimating equations or by maximizing inference objectives, like likelihoods and composite likelihoods (see, Varin et al, 2011, for a review), using user-specified templates of the estimating function or the objective functions contributions.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A key feature is the use of only those templates and forward mode automatic differentiation (as implemented in ForwardDiff) to provide methods for reduced-bias M-estimation (RBM-estimation; see, Kosmidis & Lunardon, 2020). RBM-estimation takes place either through the adjustment of the estimating equations or the penalization of the objectives, or the subtraction of an estimate of the bias of the M-estimator from the M-estimates.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"See the examples for a showcase of the functionaly GEEBRA provides.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"See NEWS.md for changes, bug fixes and enhancements.","category":"page"},{"location":"#**GEEBRA**-templates-1","page":"Home","title":"GEEBRA templates","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"GEEBRA has been designed so that the only requirements from the user are to:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"implement a Julia composite type for the data;\nimplement a function for computing the number of observations from the data object;\nimplement a function for calculating the contribution to the estimating function or to the objective function from a single observation that has arguments the parameter vector, the data object, and the observation index;\nspecify a GEEBRA template (using estimating_function_template for estimating functions and objective_function_template for objective function) that has fields the functions for computing the contributions to the estimating functions or to the objective, and the number of observations.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"GEEBRA, then, can estimate the unknown parameters by either M-estimation or RBM-estimation.","category":"page"},{"location":"#Examples-1","page":"Home","title":"Examples","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n    \"man/examples.md\",\n    ]","category":"page"},{"location":"#Documentation-1","page":"Home","title":"Documentation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n    \"lib/public.md\",\n    \"lib/internal.md\",\n    ]","category":"page"},{"location":"#main-index-1","page":"Home","title":"Index","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Pages = [\n    \"lib/public.md\",\n    \"lib/internal.md\",\n    ]","category":"page"},{"location":"#References-1","page":"Home","title":"References","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Varin, C., N. Reid, and D. Firth (2011). An overview of composite likelihood methods. Statistica Sinica 21(1), 5â€“42. Link\nKosmidis, I., N. Lunardon (2020). Empirical bias-reducing adjustments to estimating functions. ArXiv:2001.03786. Link","category":"page"},{"location":"lib/internal/#Internals-1","page":"Internal","title":"Internals","text":"","category":"section"},{"location":"lib/internal/#Contents-1","page":"Internal","title":"Contents","text":"","category":"section"},{"location":"lib/internal/#","page":"Internal","title":"Internal","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"lib/internal/#Index-1","page":"Internal","title":"Index","text":"","category":"section"},{"location":"lib/internal/#","page":"Internal","title":"Internal","text":"Pages = [\"internal.md\"]","category":"page"},{"location":"lib/internal/#Internals-2","page":"Internal","title":"Internals","text":"","category":"section"},{"location":"lib/internal/#","page":"Internal","title":"Internal","text":"GEEBRA.GEEBRA_results\nGEEBRA.show","category":"page"},{"location":"lib/internal/#GEEBRA.GEEBRA_results","page":"Internal","title":"GEEBRA.GEEBRA_results","text":"GEEBRA_results(results::Union{NLsolve.SolverResults, Optim.MultivariateOptimizationResults, Optim.UnivariateOptimizationResults}, theta::Vector, data::Any,  data::Any, template::Union{objective_function_template, estimating_function_template}, br::Bool, has_objective::Bool, br_method::String)\n\nComposite type for the output of fit for an objective_function_template or an estimating_function_template.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/#Base.show","page":"Internal","title":"Base.show","text":"show(io::IO, results::GEEBRA_results; digits::Real = 4)\n\nshow method for GEEBRA_results objects. If GEEBRA_results.has_object == true, then the result of aic(results) and tic(results) are also printed.\n\n\n\n\n\n","category":"function"},{"location":"man/examples/#Examples-1","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"man/examples/#Contents-1","page":"Examples","title":"Contents","text":"","category":"section"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Pages = [\"examples.md\"]\nDepth=3","category":"page"},{"location":"man/examples/#Ratio-of-two-means-1","page":"Examples","title":"Ratio of two means","text":"","category":"section"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Consider a setting where independent pairs of random variables (X_1 Y_1) ldots (X_n Y_n) are observed, and suppose that interest is in the ratio of the mean of Y_i to  the mean of X_i, that is theta = mu_Y  mu_X, with   mu_X = E(X_i) and mu_Y = E(Y_i) ne 0 (i = 1 ldots n).","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Assuming that sampling is from an infinite population, one way of estimating theta without any further assumptions about the joint distribution of (X_i Y_i) is to set the unbiased estimating equation sum_i = 1^n (Y_i - theta X_i) = 0. The resulting M-estimator is then  hattheta = s_Ys_X where s_X = sum_i = 1^n X_i and s_Y = sum_i = 1^n Y_i. ","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The estimator hattheta is generally biased, as can be shown, for example, by an application of the Jensen inequality assuming that X_iis independent of Y_i, and its bias can be reduced using the empirically adjusted estimating functions approach in Kosmidis & Lunardon (2020). ","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"This example illustrates how GEEBRA can be used to calculate the M-estimator and its reduced-bias version.","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"using GEEBRA, Random","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Define a data type for ratio estimation problems","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"struct ratio_data\n    y::Vector\n    x::Vector\nend;","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Write a function to compute the number of observations for objects of type ratio_data.","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"function ratio_nobs(data::ratio_data)\n    nx = length(data.x)\n    ny = length(data.y)\n    if (nx != ny) \n        error(\"length of x is not equal to the length of y\")\n    end\n    nx\nend;","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Generate some data to test things out","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Random.seed!(123);\nmy_data = ratio_data(randn(10), rand(10));\nratio_nobs(my_data)","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The estimating function for the ratio theta is ","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"sum_i = 1^n (Y_i - theta X_i)","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"So, the contribution to the estimating function can be implemented as","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"function ratio_ef(theta::Vector,\n                  data::ratio_data,\n                  i::Int64)\n    data.y[i] .- theta * data.x[i]\nend;","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The estimating_function_template for the ratio estimation problem can now be set up using ratio_nobs and ratio_ef.","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"    ratio_template = estimating_function_template(ratio_nobs, ratio_ef);","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"We are now ready use ratio_template and my_data to compute the M-estimator of theta by solving the esitmating equation sum_i = 1^n (Y_i - theta X_i) = 0. The starting value for the nonlinear solver is set to 0.1.","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"result_m = fit(ratio_template, my_data, [0.1])","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"fit uses methods from the NLsolve package for solving the estimating equations. Arguments can be passed directly to NLsolve.nlsolve through keyword arguments to the fit method. For example,","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"result_m = fit(ratio_template, my_data, [0.1], show_trace = true)","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Bias reduction in general M-estimation can be achieved by solving the adjusted estimating equation sum_i = 1^n (Y_i - theta X_i) + A(theta Y X) = 0, where A(theta) are empirical bias-reducing adjustments depending on the first and second derivatives of the estimating function contributions. GEEBRA can use ratio_template and automatic differentiation (see, ForwardDiff) to construct A(theta Y X) and, then, solve the bias-reducing adjusted estimating equations. All this is simply done by","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"result_br = fit(ratio_template, my_data, [0.1], estimation_method = \"RBM\") ","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"where RBM stands for reduced-bias M-estimation.","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Kosmidis & Lunardon (2020) show that the reduced-bias estimator of theta is tildetheta = (s_Y + s_XYs_X)(s_X + s_XXs_X). The code chunks below tests that this is indeed the result GEEBRA returns.","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"sx = sum(my_data.x);\nsxx = sum(my_data.x .* my_data.x);\nsy = sum(my_data.y);\nsxy = sum(my_data.x .* my_data.y);\nisapprox(sy/sx, result_m.theta[1])\nisapprox((sy + sxy/sx)/(sx + sxx/sx), result_br.theta[1])","category":"page"},{"location":"man/examples/#Logistic-regression-1","page":"Examples","title":"Logistic regression","text":"","category":"section"},{"location":"man/examples/#Using-[objective_function_template](@ref)-1","page":"Examples","title":"Using objective_function_template","text":"","category":"section"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Here, we use GEEBRA's objective_function_template to estimate a logistic regression model using maximum likelihood and maximum penalized likelihood, with the empirical bias-reducing penalty in Kosmidis & Lunardon (2020).","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"using GEEBRA\nusing Random\nusing Distributions\nusing Optim","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"A data type for logistic regression models (consisting of a response vector y, a model matrix x, and a vector of weights m) is","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"struct logistic_data\n    y::Vector\n    x::Array{Float64}\n    m::Vector\nend","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"A function to compute the number of observations from logistic_data objects is","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"function logistic_nobs(data::logistic_data)\n    nx = size(data.x)[1]\n    ny = length(data.y)\n    nm = length(data.m)\n    if (nx != ny) \n        error(\"number of rows in of x is not equal to the length of y\")\n    elseif (nx != nm)\n        error(\"number of rows in of x is not equal to the length of m\")\n    elseif (ny != nm)\n        error(\"length of y is not equal to the length of m\")\n    end\n    nx\nend","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The logistic regression log-likelihood contribution at a parameter theta for the ith observations of data data is","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"function logistic_loglik(theta::Vector,\n                         data::logistic_data,\n                         i::Int64)\n    eta = sum(data.x[i, :] .* theta)\n    mu = exp.(eta)./(1 .+ exp.(eta))\n    data.y[i] .* log.(mu) + (data.m[i] - data.y[i]) .* log.(1 .- mu)\nend","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Let's simulate some logistic regression data with 10 covariates","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Random.seed!(123);\nn = 100;\nm = 1;\np = 10\nx = Array{Float64}(undef, n, p);\nx[:, 1] .= 1.0;\nfor j in 2:p\n        x[:, j] .= rand(n);\nend\ntrue_betas = randn(p) * sqrt(p);\ny = rand.(Binomial.(m, cdf.(Logistic(), x * true_betas)));\nmy_data = logistic_data(y, x, fill(m, n));","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"and set up an objective_function_template for logistic regression","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"logistic_template = objective_function_template(logistic_nobs, logistic_loglik)","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The maximum likelihood estimates starting at true_betas are","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"o1_ml = fit(logistic_template, my_data, true_betas, optim_method = NelderMead())","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"fit uses methods from the Optim package internally. Here, we used the Optim.NelderMead method. Alternative optimization methods and options can be supplied directly through the keyword arguments method and optim.Options, respectively. For example,","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"o2_ml = fit(logistic_template, my_data, true_betas, optim_method = LBFGS(), optim_options = Optim.Options(g_abstol = 1e-05))","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The reduced-bias estimates starting at the maximum likelihood ones are","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"o1_br = fit(logistic_template, my_data, coef(o1_ml), estimation_method = \"RBM\")","category":"page"},{"location":"man/examples/#Using-[estimating_function_template](@ref)-1","page":"Examples","title":"Using estimating_function_template","text":"","category":"section"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The same results as above can be returned using an estimating_function_template for logistic regression. ","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The contribution to the derivatives of the log-likelihood for logistic regression is","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"function logistic_ef(theta::Vector,\n                     data::logistic_data,\n                     i::Int64)\n    eta = sum(data.x[i, :] .* theta)\n    mu = exp.(eta)./(1 .+ exp.(eta))\n    data.x[i, :] * (data.y[i] - data.m[i] * mu)\nend","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"Then, solving the bias-reducing adjusted estimating equations","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"logistic_template_ef = estimating_function_template(logistic_nobs, logistic_ef);\ne1_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = \"RBM\")","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"returns the reduced-bias estimates from maximum penalized likelihood:","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"isapprox(coef(o1_br), coef(e1_br))","category":"page"},{"location":"man/examples/#Bias-reduction-methods-1","page":"Examples","title":"Bias-reduction methods","text":"","category":"section"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"GEEBRA currently implements 2 alternative bias reduction methods, called implicit_trace and explicit_trace. implicit_trace will adjust the estimating functions or penalize the objectives, as we have seen earlier. explicit_trace, on the other hand, will form an estimate of the bias of the M-estimator and subtract that from the M-estimates. The default method is implicit_trace.","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"For example, for logistic regression via estimating functions ","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"e2_br = fit(logistic_template_ef, my_data, true_betas, estimation_method = \"RBM\", br_method = \"explicit_trace\")","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"which gives slightly different estimates that what are in the implict_trace fit in e1_br. ","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"The same can be done using objective functions, but numerical differentiation (using the FiniteDiff package) is used to approximate the gradient of the bias-reducing penalty (i.e. A(theta)).","category":"page"},{"location":"man/examples/#","page":"Examples","title":"Examples","text":"o2_br = fit(logistic_template, my_data, true_betas, estimation_method = \"RBM\", br_method = \"explicit_trace\")\nisapprox(coef(e2_br), coef(o2_br))","category":"page"}]
}
